"""
EBSCO Data Cleaner - Limpieza y Deduplicaci√≥n de Datos Acad√©micos
===================================================================

Este m√≥dulo proporciona herramientas para limpiar, normalizar y eliminar
duplicados de datasets extra√≠dos de la base de datos acad√©mica EBSCO.

El proceso de limpieza incluye:
- Eliminaci√≥n de t√≠tulos vac√≠os o inv√°lidos
- Detecci√≥n y eliminaci√≥n inteligente de duplicados
- Normalizaci√≥n de texto (espacios, caracteres especiales)
- Generaci√≥n de reportes detallados de limpieza
- Exportaci√≥n a m√∫ltiples formatos con informaci√≥n de trazabilidad

Criterios de Duplicaci√≥n:
--------------------------
Se considera que dos registros son duplicados si comparten:
1. T√≠tulo normalizado (sin puntuaci√≥n, min√∫sculas, sin espacios extra)
2. DOI (Digital Object Identifier) - si est√° disponible
3. Autores normalizados

Archivos Generados:
-------------------
1. *_LIMPIO.csv: Dataset final sin duplicados ni registros inv√°lidos
2. *_COMPLETO.csv: Dataset original con columna indicando registros eliminados
3. *_REPORTE.txt: Reporte detallado con estad√≠sticas de limpieza

Workflow T√≠pico:
----------------
1. Cargar datos desde CSV
2. Limpiar texto y normalizar campos
3. Identificar duplicados usando hash MD5
4. Eliminar duplicados manteniendo el primer registro de cada grupo
5. Generar reportes y exportar resultados

Autor: [Tu nombre]
Fecha: 2025
Licencia: [Tu licencia]
"""

import pandas as pd
import re
from typing import List, Dict, Tuple, Optional, Any
import hashlib
from datetime import datetime
import os


class DataCleaner:
    """
    Clase principal para limpieza y deduplicaci√≥n de datos de EBSCO.
    
    Esta clase implementa un pipeline completo de limpieza de datos que
    procesa archivos CSV de EBSCO, elimina duplicados usando algoritmos
    de hash, normaliza texto y genera reportes detallados.
    
    La clase mantiene dos versiones del dataset:
    1. df_original: Dataset original sin modificar
    2. df_clean: Dataset limpio despu√©s del procesamiento
    
    Adem√°s, almacena metadata completa sobre qu√© registros fueron eliminados
    y por qu√© raz√≥n, permitiendo trazabilidad total del proceso.
    
    Attributes:
        input_file (str): Ruta al archivo CSV de entrada
        df_original (Optional[pd.DataFrame]): DataFrame con datos originales
        df_clean (Optional[pd.DataFrame]): DataFrame con datos limpios
        duplicate_info (Dict[int, Dict[str, Any]]): Informaci√≥n sobre registros
            duplicados eliminados. Key: √≠ndice del registro eliminado,
            Value: dict con raz√≥n, √≠ndice conservado, etc.
        cleaning_stats (Dict[str, int]): Estad√≠sticas del proceso de limpieza
            incluyendo conteos de registros originales, finales, eliminados.
    
    Example:
        >>> cleaner = DataCleaner("ebsco_articles.csv")
        >>> cleaner.load_data()
        >>> df_clean = cleaner.clean_data()
        >>> cleaner.save_files("articles_cleaned")
        
        >>> # An√°lisis de duplicados
        >>> dup_analysis = cleaner.get_duplicate_analysis()
        >>> print(dup_analysis.head())
    
    Note:
        Los DataFrames se inicializan como None y se cargan/generan
        mediante los m√©todos load_data() y clean_data() respectivamente.
    """
    
    def __init__(self, input_file: str):
        """
        Inicializa el limpiador de datos con un archivo CSV de entrada.
        
        Configura las estructuras de datos necesarias para el proceso de
        limpieza, incluyendo DataFrames, diccionarios de informaci√≥n de
        duplicados y contadores de estad√≠sticas.
        
        Args:
            input_file (str): Ruta al archivo CSV que contiene los datos
                de EBSCO a limpiar. Debe ser un CSV v√°lido con encoding UTF-8
                y debe incluir al menos la columna 'title'.
        
        Initializes:
            - df_original: None (se carga con load_data())
            - df_clean: None (se genera con clean_data())
            - duplicate_info: Diccionario vac√≠o para metadata de duplicados
            - cleaning_stats: Diccionario con contadores en 0
        
        Note:
            Este m√©todo solo inicializa estructuras, no carga los datos.
            Debe llamarse load_data() expl√≠citamente despu√©s de la inicializaci√≥n.
        
        Example:
            >>> cleaner = DataCleaner("articles.csv")
            >>> # Ahora se debe llamar cleaner.load_data()
        """
        self.input_file = input_file
        
        # DataFrames principales (se inicializan como None hasta load_data / clean_data)
        self.df_original: Optional[pd.DataFrame] = None
        self.df_clean: Optional[pd.DataFrame] = None
        
        # Diccionario de informaci√≥n de duplicados
        # Key: √≠ndice del registro eliminado
        # Value: diccionario con metadata (raz√≥n, √≠ndice conservado, t√≠tulo, etc.)
        self.duplicate_info: Dict[int, Dict[str, Any]] = {}
        
        # Estad√≠sticas de limpieza
        self.cleaning_stats: Dict[str, int] = {
            'original_count': 0,              # N√∫mero de registros originales
            'clean_count': 0,                 # N√∫mero de registros despu√©s de limpiar
            'duplicates_removed': 0,          # Cantidad de duplicados eliminados
            'empty_titles_removed': 0,        # Registros sin t√≠tulo eliminados
            'invalid_records_removed': 0      # Otros registros inv√°lidos
        }
        
    def load_data(self) -> bool:
        """
        Carga los datos desde el archivo CSV especificado en __init__.
        
        Lee el archivo CSV usando pandas con encoding UTF-8 y almacena el
        resultado en df_original. Tambi√©n inicializa el contador de registros
        originales en las estad√≠sticas de limpieza.
        
        Returns:
            bool: True si la carga fue exitosa, False si hubo alg√∫n error
                (archivo no encontrado, formato inv√°lido, encoding incorrecto, etc.)
        
        Side Effects:
            - Popula self.df_original con el DataFrame cargado
            - Actualiza self.cleaning_stats['original_count']
            - Imprime mensaje de √©xito o error en consola
        
        Raises:
            No lanza excepciones - captura todos los errores y retorna False.
            Los errores se imprimen en consola para debugging.
        
        Example:
            >>> cleaner = DataCleaner("articles.csv")
            >>> if cleaner.load_data():
            ...     print("Datos cargados exitosamente")
            ...     print(f"Columnas: {cleaner.df_original.columns.tolist()}")
            ... else:
            ...     print("Error al cargar datos")
            ‚úÖ Datos cargados exitosamente: 1,234 registros
        
        Note:
            Asume que el CSV usa encoding UTF-8. Si tu archivo usa otro
            encoding (ej: latin-1, iso-8859-1), modifica el par√°metro encoding.
        """
        try:
            # Cargar CSV con pandas
            self.df_original = pd.read_csv(self.input_file, encoding='utf-8')
            
            # Actualizar contador de registros originales
            self.cleaning_stats['original_count'] = len(self.df_original)
            
            # Mensaje de √©xito con formato de miles
            print(f"Datos cargados exitosamente: {len(self.df_original):,} registros")
            return True
            
        except Exception as e:
            # Capturar cualquier error y mostrar mensaje
            print(f"Error cargando el archivo: {e}")
            return False
    
    def clean_text(self, text: str) -> str:
        """
        Limpia y normaliza texto eliminando caracteres especiales.
        
        Realiza m√∫ltiples transformaciones al texto para normalizarlo:
        - Convierte a string si es otro tipo
        - Elimina saltos de l√≠nea, retornos de carro y tabulaciones
        - Normaliza m√∫ltiples espacios a un solo espacio
        - Elimina espacios al inicio y final
        
        Este m√©todo es √∫til para preparar campos de texto antes de
        comparaciones o almacenamiento, asegurando consistencia.
        
        Args:
            text (str): Texto a limpiar. Puede ser cualquier tipo que
                sea convertible a string.
        
        Returns:
            str: Texto limpio y normalizado. Retorna string vac√≠o "" si
                el input es None, NaN o string vac√≠o.
        
        Process:
            1. Verificar si es NaN o vac√≠o ‚Üí retornar ""
            2. Convertir a string si no lo es
            3. Reemplazar \\r, \\n, \\t con espacios
            4. Colapsar m√∫ltiples espacios a uno solo
            5. Eliminar espacios iniciales y finales
        
        Example:
            >>> cleaner = DataCleaner("data.csv")
            >>> text = "Hello\\n\\nWorld\\t\\tTest   Multiple  Spaces  "
            >>> cleaned = cleaner.clean_text(text)
            >>> print(f"'{cleaned}'")
            'Hello World Test Multiple Spaces'
            
            >>> # Maneja NaN y valores vac√≠os
            >>> cleaner.clean_text(None)
            ''
            >>> cleaner.clean_text("")
            ''
            >>> cleaner.clean_text(float('nan'))
            ''
        
        Note:
            Este m√©todo NO elimina puntuaci√≥n ni convierte a min√∫sculas.
            Para eso, ver normalize_title().
        """
        # Verificar si el texto es NaN o vac√≠o
        if pd.isna(text) or text == "":
            return ""
        
        # Convertir a string si no lo es (ej: n√∫meros, fechas, etc.)
        text = str(text)
        
        # Eliminar caracteres de control (\\r, \\n, \\t) y reemplazar con espacio
        text = re.sub(r'[\r\n\t]+', ' ', text)
        
        # Normalizar m√∫ltiples espacios a un solo espacio
        text = re.sub(r'\s+', ' ', text)
        
        # Eliminar espacios al inicio y final
        text = text.strip()
        
        return text
    
    def normalize_title(self, title: str) -> str:
        """
        Normaliza t√≠tulos para comparaci√≥n de duplicados.
        
        Aplica transformaciones agresivas al t√≠tulo para permitir detecci√≥n
        de duplicados incluso cuando hay peque√±as diferencias de formato,
        puntuaci√≥n o capitalizaci√≥n.
        
        El t√≠tulo normalizado se usa SOLO para comparaci√≥n, no se guarda
        en el dataset final. El t√≠tulo original se mantiene intacto.
        
        Args:
            title (str): T√≠tulo original del art√≠culo
        
        Returns:
            str: T√≠tulo normalizado en min√∫sculas, sin puntuaci√≥n ni
                caracteres especiales, con espacios normalizados. Retorna
                string vac√≠o si el input es None, NaN o vac√≠o.
        
        Transformations:
            1. Convertir a min√∫sculas
            2. Eliminar TODA la puntuaci√≥n y caracteres especiales
            3. Mantener solo caracteres alfanum√©ricos y espacios
            4. Normalizar espacios m√∫ltiples a uno solo
            5. Eliminar espacios iniciales y finales
        
        Example:
            >>> cleaner = DataCleaner("data.csv")
            
            >>> # T√≠tulos similares se normalizan igual
            >>> t1 = "Machine Learning: A Comprehensive Guide"
            >>> t2 = "machine learning a comprehensive guide"
            >>> t3 = "Machine Learning - A Comprehensive Guide!"
            >>> 
            >>> cleaner.normalize_title(t1)
            'machine learning a comprehensive guide'
            >>> cleaner.normalize_title(t2)
            'machine learning a comprehensive guide'
            >>> cleaner.normalize_title(t3)
            'machine learning a comprehensive guide'
            >>> # Los tres se consideran duplicados
        
        Use Cases:
            - Detecci√≥n de duplicados con peque√±as variaciones
            - Comparaci√≥n de t√≠tulos case-insensitive
            - Matching fuzzy de t√≠tulos similares
        
        Note:
            Esta normalizaci√≥n es MUY agresiva. T√≠tulos genuinamente
            diferentes pero con palabras similares podr√≠an colisionar.
            Por eso se combina con DOI y autores para mejor precisi√≥n.
        """
        # Verificar si el t√≠tulo es NaN o vac√≠o
        if pd.isna(title) or title == "":
            return ""
        
        # Convertir a string y luego a min√∫sculas
        normalized = str(title).lower()
        
        # Eliminar TODA la puntuaci√≥n y caracteres especiales
        # Mantener solo: letras, n√∫meros, espacios
        # [^\w\s] significa: todo lo que NO sea word character (letras, n√∫meros, _) ni espacios
        normalized = re.sub(r'[^\w\s]', '', normalized)
        
        # Normalizar espacios m√∫ltiples a un solo espacio
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        return normalized
    
    def create_duplicate_key(self, row: pd.Series) -> str:
        """
        Crea una clave √∫nica (hash) para identificar registros duplicados.
        
        Genera un hash MD5 basado en la combinaci√≥n de t√≠tulo normalizado,
        DOI y autores. Dos registros con el mismo hash se consideran duplicados.
        
        El hash permite comparaciones r√°pidas (O(1)) en lugar de comparaciones
        string por string (O(n)) para cada par de registros.
        
        Args:
            row (pd.Series): Fila del DataFrame que representa un art√≠culo.
                Debe contener (idealmente) las columnas: 'title', 'doi', 'authors'
        
        Returns:
            str: Hash MD5 de 32 caracteres hexadecimales que identifica
                √∫nicamente la combinaci√≥n de t√≠tulo, DOI y autores.
        
        Algorithm:
            1. Normalizar t√≠tulo (min√∫sculas, sin puntuaci√≥n)
            2. Normalizar DOI (min√∫sculas, sin espacios)
            3. Normalizar autores (min√∫sculas, sin espacios)
            4. Concatenar con pipe "|" como separador
            5. Generar hash MD5 de la string concatenada
        
        Hash Collisions:
            MD5 tiene probabilidad casi nula de colisi√≥n para este caso de uso.
            Si dos registros diferentes generan el mismo hash (extremadamente
            raro), se considerar√≠an err√≥neamente duplicados. En la pr√°ctica,
            esto no ocurre con datasets acad√©micos.
        
        Example:
            >>> cleaner = DataCleaner("data.csv")
            >>> row1 = pd.Series({
            ...     'title': 'Machine Learning: An Introduction',
            ...     'doi': '10.1234/ml.2023.001',
            ...     'authors': 'John Smith; Jane Doe'
            ... })
            >>> 
            >>> key1 = cleaner.create_duplicate_key(row1)
            >>> print(len(key1))  # Hash MD5 es siempre 32 caracteres
            32
            >>> print(key1[:8])  # Primeros 8 caracteres
            'a3f7b2c1'
            >>> 
            >>> # Mismo contenido ‚Üí mismo hash
            >>> row2 = pd.Series({
            ...     'title': 'MACHINE LEARNING - AN INTRODUCTION!!!',
            ...     'doi': '10.1234/ml.2023.001',
            ...     'authors': 'john smith; jane doe'
            ... })
            >>> key2 = cleaner.create_duplicate_key(row2)
            >>> key1 == key2  # Son duplicados
            True
        
        Performance:
            Generaci√≥n de hash es O(1) y muy r√°pida. Permite detectar
            duplicados en datasets de millones de registros eficientemente.
        
        Note:
            Si los campos title, doi o authors no existen en row, se usan
            strings vac√≠os. Registros sin informaci√≥n podr√≠an generar
            hashes similares err√≥neamente.
        """
        # Obtener y normalizar t√≠tulo
        title = self.normalize_title(row.get('title', ''))
        
        # Obtener y normalizar DOI (convertir a min√∫sculas y eliminar espacios)
        doi = str(row.get('doi', '')).lower().strip()
        
        # Obtener y normalizar autores (convertir a min√∫sculas y eliminar espacios)
        authors = str(row.get('authors', '')).lower().strip()
        
        # Crear string combinada usando pipe como separador
        # Formato: "titulo|doi|autores"
        combined = f"{title}|{doi}|{authors}"
        
        # Generar hash MD5 (32 caracteres hexadecimales)
        hash_object = hashlib.md5(combined.encode('utf-8'))
        return hash_object.hexdigest()
    
    def identify_duplicates(self, df: Optional[pd.DataFrame] = None) -> Dict[str, List[int]]:
        """
        Identifica grupos de registros duplicados en el DataFrame.
        
        Itera sobre todos los registros del DataFrame, genera una clave hash
        para cada uno y agrupa registros con la misma clave. Retorna solo
        los grupos que tienen m√°s de un registro (duplicados reales).
        
        Args:
            df (Optional[pd.DataFrame], optional): DataFrame a analizar.
                Si es None, usa self.df_original. Por defecto None.
        
        Returns:
            Dict[str, List[int]]: Diccionario donde:
                - Key: Hash MD5 que identifica el grupo de duplicados
                - Value: Lista de √≠ndices de registros que comparten ese hash
                Solo incluye grupos con 2+ registros (duplicados reales).
        
        Raises:
            ValueError: Si no hay DataFrame cargado (df is None y 
                self.df_original is None)
        
        Algorithm:
            1. Para cada fila en el DataFrame:
                a. Generar clave hash usando create_duplicate_key()
                b. Agregar √≠ndice a la lista del hash correspondiente
            2. Filtrar solo grupos con len(lista) > 1
            3. Calcular estad√≠sticas (total de duplicados a eliminar)
        
        Performance:
            - Tiempo: O(n) donde n = n√∫mero de registros
            - Espacio: O(d) donde d = n√∫mero de duplicados
            - Muy eficiente incluso con millones de registros
        
        Example:
            >>> cleaner = DataCleaner("articles.csv")
            >>> cleaner.load_data()
            >>> duplicates = cleaner.identify_duplicates()
            üîç Identificando duplicados...
            üìä Encontrados 45 grupos de duplicados
            üìä Total de registros duplicados a eliminar: 123
            
            >>> # Analizar un grupo espec√≠fico
            >>> first_group_key = list(duplicates.keys())[0]
            >>> indices = duplicates[first_group_key]
            >>> print(f"Grupo con {len(indices)} duplicados en √≠ndices: {indices}")
            Grupo con 3 duplicados en √≠ndices: [42, 156, 789]
            
            >>> # Ver t√≠tulos de ese grupo
            >>> for idx in indices:
            ...     title = cleaner.df_original.iloc[idx]['title']
            ...     print(f"  [{idx}] {title[:50]}...")
        
        Duplicate Elimination Strategy:
            De cada grupo de duplicados, se MANTIENE el primero (√≠ndice m√°s bajo)
            y se ELIMINAN todos los dem√°s. Esto se hace en clean_data().
            
            Ejemplo: Si el grupo es [42, 156, 789]:
            - Se mantiene el registro 42
            - Se eliminan los registros 156 y 789
        
        Note:
            Este m√©todo solo IDENTIFICA duplicados, no los elimina.
            La eliminaci√≥n se hace en clean_data().
        """
        print("üîç Identificando duplicados...")
        
        # Determinar qu√© DataFrame usar
        if df is None:
            df = self.df_original
        if df is None:
            raise ValueError("No hay DataFrame cargado para detecci√≥n de duplicados.")

        # Diccionario para agrupar √≠ndices por hash
        duplicate_groups: Dict[str, List[int]] = {}

        # Iterar sobre cada fila del DataFrame
        for idx, row in df.iterrows():  # type: ignore[union-attr]
            # Generar clave hash para esta fila
            key: str = self.create_duplicate_key(row)
            
            # Inicializar lista para este hash si no existe
            if key not in duplicate_groups:
                duplicate_groups[key] = []
            
            # Agregar √≠ndice a la lista de este hash
            duplicate_groups[key].append(idx)  # type: ignore[arg-type]

        # Filtrar solo grupos con duplicados (m√°s de 1 registro)
        duplicates = {k: v for k, v in duplicate_groups.items() if len(v) > 1}

        # Mostrar estad√≠sticas
        print(f"Encontrados {len(duplicates)} grupos de duplicados")

        # Calcular total de registros que ser√°n eliminados
        # De cada grupo, se elimina len(grupo) - 1 registros
        total_duplicates = sum(len(group) - 1 for group in duplicates.values())
        print(f"Total de registros duplicados a eliminar: {total_duplicates}")

        return duplicates
    
    def clean_data(self) -> pd.DataFrame:
        """
        Ejecuta el pipeline completo de limpieza de datos.
        
        Este es el m√©todo principal que orquesta todo el proceso de limpieza:
        1. Limpia texto en columnas principales
        2. Elimina registros con t√≠tulos vac√≠os
        3. Identifica duplicados usando hashing
        4. Elimina duplicados manteniendo primer registro de cada grupo
        5. Resetea √≠ndices del DataFrame
        6. Calcula y almacena estad√≠sticas
        
        Returns:
            pd.DataFrame: DataFrame limpio sin duplicados ni registros inv√°lidos.
                Tambi√©n se almacena en self.df_clean para acceso posterior.
        
        Raises:
            ValueError: Si load_data() no se ha ejecutado previamente
                (self.df_original is None)
        
        Side Effects:
            - Modifica self.df_clean (lo crea/actualiza)
            - Modifica self.duplicate_info (guarda metadata de eliminados)
            - Modifica self.cleaning_stats (actualiza contadores)
            - Imprime mensajes de progreso en consola
        
        Process Flow:
            PASO 1: Limpiar Texto
            ‚îú‚îÄ Para cada columna de texto (title, abstract, authors, etc.)
            ‚îî‚îÄ Aplicar clean_text() para normalizar espacios y caracteres
            
            PASO 2: Eliminar T√≠tulos Vac√≠os
            ‚îú‚îÄ Filtrar registros donde title.strip() == ''
            ‚îî‚îÄ Actualizar cleaning_stats['empty_titles_removed']
            
            PASO 3: Identificar Duplicados
            ‚îú‚îÄ Llamar identify_duplicates() con DataFrame filtrado
            ‚îî‚îÄ Obtener grupos de √≠ndices duplicados
            
            PASO 4: Preparar Eliminaci√≥n de Duplicados
            ‚îú‚îÄ Para cada grupo de duplicados:
            ‚îÇ  ‚îú‚îÄ Mantener primer √≠ndice (menor)
            ‚îÇ  ‚îú‚îÄ Marcar resto para eliminaci√≥n
            ‚îÇ  ‚îî‚îÄ Guardar metadata en duplicate_info
            ‚îî‚îÄ Crear lista completa de √≠ndices a eliminar
            
            PASO 5: Eliminar y Resetear
            ‚îú‚îÄ Eliminar filas usando df.drop()
            ‚îú‚îÄ Resetear √≠ndices (reset_index)
            ‚îî‚îÄ Actualizar cleaning_stats
        
        Example:
            >>> cleaner = DataCleaner("ebsco_data.csv")
            >>> cleaner.load_data()
            ‚úÖ Datos cargados exitosamente: 5,432 registros
            
            >>> df_clean = cleaner.clean_data()
            üßπ Iniciando limpieza de datos...
            üóëÔ∏è Eliminados 12 registros con t√≠tulos vac√≠os
            üîç Identificando duplicados...
            üìä Encontrados 87 grupos de duplicados
            üìä Total de registros duplicados a eliminar: 234
            ‚úÖ Limpieza completada:
               üìä Registros originales: 5,432
               üìä T√≠tulos vac√≠os eliminados: 12
               üìä Duplicados eliminados: 234
               üìä Registros finales: 5,186
            
            >>> # Acceder al DataFrame limpio
            >>> print(f"Shape: {df_clean.shape}")
            >>> print(f"Columnas: {df_clean.columns.tolist()}")
        
        Duplicate Handling:
            De cada grupo de duplicados, se mantiene el registro con el
            √≠ndice m√°s bajo (primer registro encontrado) y se eliminan
            todos los dem√°s. Esto asegura que siempre hay un registro
            representativo mantenido.
        
        Metadata Tracking:
            Para cada registro eliminado, se guarda en duplicate_info:
            - reason: 'DUPLICADO'
            - kept_index: √çndice del registro que se mantuvo
            - duplicate_of_title: Primeros 100 chars del t√≠tulo mantenido
            
            Esto permite trazabilidad completa de qu√© se elimin√≥ y por qu√©.
        
        Performance:
            - Tiempo: O(n) donde n = n√∫mero de registros
            - Espacio: O(d) donde d = n√∫mero de duplicados
            - Eficiente para datasets de millones de registros
        
        Note:
            Este m√©todo NO modifica df_original. Trabaja en una copia
            y genera un nuevo DataFrame limpio.
        """
        print("Iniciando limpieza de datos...")
        
        # Verificar que los datos est√°n cargados
        if self.df_original is None:
            raise ValueError("Datos no cargados. Ejecuta load_data() primero.")
        
        # ===== PASO 1: CREAR COPIA DE TRABAJO =====
        df_work: pd.DataFrame = self.df_original.copy()
        
        # ===== PASO 2: LIMPIAR TEXTO EN COLUMNAS PRINCIPALES =====
        text_columns = ['title', 'abstract', 'authors', 'journal', 'subjects']
        for col in text_columns:
            if col in df_work.columns:
                # Aplicar clean_text a cada celda de la columna
                df_work[col] = df_work[col].apply(self.clean_text)
        
        # ===== PASO 3: ELIMINAR REGISTROS CON T√çTULOS VAC√çOS =====
        initial_count = len(df_work)
        
        # Filtrar registros donde el t√≠tulo no est√° vac√≠o despu√©s de strip()
        df_work = df_work.loc[df_work['title'].str.strip() != '']  # type: ignore[assignment]
        
        # Calcular cu√°ntos se eliminaron
        empty_titles_removed = initial_count - len(df_work)
        self.cleaning_stats['empty_titles_removed'] = empty_titles_removed
        
        if empty_titles_removed > 0:
            print(f"Eliminados {empty_titles_removed} registros con t√≠tulos vac√≠os")
        
        # ===== PASO 4: IDENTIFICAR DUPLICADOS =====
        duplicates = self.identify_duplicates(df_work)
        
        # Listas para almacenar informaci√≥n de eliminaci√≥n
        indices_to_remove = []
        duplicate_info = {}
        
        # ===== PASO 5: PREPARAR ELIMINACI√ìN DE DUPLICADOS =====
        for dup_key, indices in duplicates.items():
            if len(indices) > 1:
                # Estrategia: mantener el PRIMERO, eliminar el resto
                keep_idx = indices[0]           # √çndice a mantener (primer registro)
                remove_indices = indices[1:]    # √çndices a eliminar (resto)
                
                # Para cada √≠ndice a eliminar, guardar metadata
                for remove_idx in remove_indices:
                    indices_to_remove.append(remove_idx)
                    
                    # Obtener t√≠tulo del registro que se mantiene
                    kept_title_val = df_work.loc[keep_idx, 'title'] if 'title' in df_work.columns else ''
                    kept_title_str = '' if pd.isna(kept_title_val) else str(kept_title_val)
                    
                    # Truncar t√≠tulo a 100 caracteres para metadata
                    short_kept_title = kept_title_str[:100] + ("..." if len(kept_title_str) > 100 else "")
                    
                    # Guardar informaci√≥n del duplicado eliminado
                    duplicate_info[remove_idx] = {
                        'reason': 'DUPLICADO',
                        'kept_index': keep_idx,
                        'duplicate_of_title': short_kept_title
                    }
        
        # ===== PASO 6: ELIMINAR DUPLICADOS DEL DATAFRAME =====
        df_work = df_work.drop(indices_to_remove, errors='ignore')
        self.cleaning_stats['duplicates_removed'] = len(indices_to_remove)
        
        # ===== PASO 7: RESETEAR √çNDICES =====
        # Despu√©s de eliminar filas, los √≠ndices quedan discontinuos
        # reset_index los hace continuos desde 0
        df_work = df_work.reset_index(drop=True)
        
        # ===== PASO 8: GUARDAR INFORMACI√ìN Y ESTAD√çSTICAS =====
        self.duplicate_info = duplicate_info
        self.cleaning_stats['clean_count'] = len(df_work)
        
        # Mostrar resumen de limpieza
        print("Limpieza completada:")
        print(f"   Registros originales: {self.cleaning_stats['original_count']:,}")
        print(f"   T√≠tulos vac√≠os eliminados: {self.cleaning_stats['empty_titles_removed']:,}")
        print(f"   Duplicados eliminados: {self.cleaning_stats['duplicates_removed']:,}")
        print(f"   Registros finales: {self.cleaning_stats['clean_count']:,}")

        # Guardar DataFrame limpio en la instancia
        self.df_clean = df_work
        return df_work
    
    def create_removal_info_column(self) -> pd.DataFrame:
        """
        Crea versi√≥n del DataFrame original con columna de informaci√≥n de eliminaci√≥n.
        
        Genera una nueva columna 'removal_info' en el DataFrame original que indica
        para cada registro si fue CONSERVADO o ELIMINADO, y en caso de eliminaci√≥n,
        la raz√≥n espec√≠fica (t√≠tulo vac√≠o, duplicado, etc.).
        
        Esta funcionalidad es √∫til para:
        - Auditor√≠a del proceso de limpieza
        - Trazabilidad de decisiones
        - An√°lisis de qu√© registros fueron eliminados
        - Recuperaci√≥n manual de registros si es necesario
        
        Returns:
            pd.DataFrame: Copia del DataFrame original con nueva columna 'removal_info'
                que contiene una de las siguientes etiquetas:
                - "CONSERVADO": Registro se mantuvo en dataset limpio
                - "ELIMINADO - T√çTULO VAC√çO": Registro sin t√≠tulo v√°lido
                - "ELIMINADO - DUPLICADO: Duplicado del √≠ndice X": Es copia de otro registro
        
        Raises:
            ValueError: Si clean_data() no se ha ejecutado previamente
                (self.df_original is None)
        
        Column Values:
            - "CONSERVADO": Registros que pasaron todos los filtros
            - "ELIMINADO - T√çTULO VAC√çO": title.strip() == ''
            - "ELIMINADO - DUPLICADO: Duplicado del √≠ndice X": 
              Donde X es el √≠ndice del registro original que se mantuvo
        
        Example:
            >>> cleaner = DataCleaner("articles.csv")
            >>> cleaner.load_data()
            >>> cleaner.clean_data()
            
            >>> df_with_info = cleaner.create_removal_info_column()
            >>> 
            >>> # Ver distribuci√≥n de estados
            >>> print(df_with_info['removal_info'].value_counts())
            CONSERVADO                                  4,567
            ELIMINADO - DUPLICADO: Duplicado del...      234
            ELIMINADO - T√çTULO VAC√çO                      12
            
            >>> # Ver solo registros eliminados
            >>> removed = df_with_info[df_with_info['removal_info'] != 'CONSERVADO']
            >>> print(removed[['title', 'removal_info']].head())
        
        Use Cases:
            1. **Auditor√≠a**: Revisar qu√© y por qu√© se elimin√≥
            2. **Recuperaci√≥n**: Encontrar registros eliminados por error
            3. **Estad√≠sticas**: Analizar patrones de duplicaci√≥n
            4. **Documentaci√≥n**: Evidencia de proceso de limpieza
        
        Note:
            Este m√©todo NO modifica self.df_original. Retorna una nueva copia.
            Para guardar esta versi√≥n, usar save_files() que autom√°ticamente
            genera el archivo *_COMPLETO.csv con esta informaci√≥n.
        """
        # Verificar que df_original existe
        if self.df_original is None:
            raise ValueError("df_original es None. Llama a load_data() antes de create_removal_info_column().")

        # Crear copia del DataFrame original
        df_with_info = self.df_original.copy()
        
        # Inicializar columna removal_info con strings vac√≠os
        df_with_info['removal_info'] = ''
        
        # ===== PASO 1: MARCAR DUPLICADOS ELIMINADOS =====
        for idx, info in self.duplicate_info.items():
            # Verificar que el √≠ndice existe en el DataFrame
            if idx < len(df_with_info):
                # Crear mensaje descriptivo con raz√≥n e √≠ndice conservado
                removal_message = (
                    f"ELIMINADO - {info['reason']}: "
                    f"Duplicado del √≠ndice {info['kept_index']}"
                )
                df_with_info.loc[idx, 'removal_info'] = removal_message
        
        # ===== PASO 2: MARCAR T√çTULOS VAC√çOS =====
        # Identificar registros con t√≠tulos vac√≠os (despu√©s de strip)
        empty_title_mask = df_with_info['title'].str.strip() == ''
        df_with_info.loc[empty_title_mask, 'removal_info'] = 'ELIMINADO - T√çTULO VAC√çO'
        
        # ===== PASO 3: MARCAR REGISTROS CONSERVADOS =====
        # Todos los registros que no tienen removal_info son conservados
        keep_mask = df_with_info['removal_info'] == ''
        df_with_info.loc[keep_mask, 'removal_info'] = 'CONSERVADO'
        
        return df_with_info
    
    def generate_cleaning_report(self) -> str:
        """
        Genera un reporte de texto detallado del proceso de limpieza.
        
        Crea un reporte formateado con todas las estad√≠sticas del proceso
        de limpieza, incluyendo conteos, porcentajes y criterios utilizados.
        Este reporte es √∫til para documentaci√≥n y auditor√≠a.
        
        Returns:
            str: Reporte de limpieza formateado con m√∫ltiples secciones:
                - Fecha y hora de generaci√≥n
                - Archivo procesado
                - Estad√≠sticas generales (originales, finales, eliminados)
                - Detalle por tipo de eliminaci√≥n
                - Tasas porcentuales de conservaci√≥n/eliminaci√≥n
                - Criterios de duplicaci√≥n utilizados
        
        Report Sections:
            1. **Metadata**: Fecha, hora, archivo procesado
            2. **Estad√≠sticas Generales**: Conteos totales
            3. **Detalle de Eliminaciones**: Por categor√≠a
            4. **Tasas Porcentuales**: % conservado vs eliminado
            5. **Criterios**: C√≥mo se identificaron duplicados
        
        Example:
            >>> cleaner = DataCleaner("articles.csv")
            >>> cleaner.load_data()
            >>> cleaner.clean_data()
            >>> 
            >>> report = cleaner.generate_cleaning_report()
            >>> print(report)
            
            === REPORTE DE LIMPIEZA DE DATOS EBSCO ===
            Fecha: 2025-01-15 14:30:45
            Archivo procesado: articles.csv
            
            ESTAD√çSTICAS:
            - Registros originales: 5,432
            - Registros finales (limpios): 5,186
            - Total eliminados: 246
            
            DETALLE DE ELIMINACIONES:
            - T√≠tulos vac√≠os: 12
            - Duplicados: 234
            
            TASA DE LIMPIEZA:
            - Porcentaje conservado: 95.47%
            - Porcentaje eliminado: 4.53%
            
            CRITERIOS DE DUPLICACI√ìN:
            - T√≠tulo normalizado (sin puntuaci√≥n, min√∫sculas)
            - DOI (si est√° disponible)
            - Autores
            ========================================
        
        Use Cases:
            - **Documentaci√≥n**: Anexar a informes de proyecto
            - **Auditor√≠a**: Evidencia de proceso de limpieza
            - **Versionamiento**: Registrar cada limpieza realizada
            - **An√°lisis**: Comparar resultados entre diferentes datasets
        
        Note:
            El reporte se genera basado en cleaning_stats que se popula
            durante clean_data(). Debe ejecutarse clean_data() antes de
            generar el reporte.
        """
        # Calcular totales y porcentajes
        total_removed = self.cleaning_stats['original_count'] - self.cleaning_stats['clean_count']
        
        # Calcular porcentajes con protecci√≥n contra divisi√≥n por cero
        if self.cleaning_stats['original_count'] > 0:
            pct_conserved = (self.cleaning_stats['clean_count'] / 
                           self.cleaning_stats['original_count'] * 100)
            pct_removed = (total_removed / 
                         self.cleaning_stats['original_count'] * 100)
        else:
            pct_conserved = 0.0
            pct_removed = 0.0
        
        # Construir reporte formateado
        report = f"""
        === REPORTE DE LIMPIEZA DE DATOS EBSCO ===
        Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        Archivo procesado: {self.input_file}

        ESTAD√çSTICAS:
        - Registros originales: {self.cleaning_stats['original_count']:,}
        - Registros finales (limpios): {self.cleaning_stats['clean_count']:,}
        - Total eliminados: {total_removed:,}

        DETALLE DE ELIMINACIONES:
        - T√≠tulos vac√≠os: {self.cleaning_stats['empty_titles_removed']:,}
        - Duplicados: {self.cleaning_stats['duplicates_removed']:,}

        TASA DE LIMPIEZA:
        - Porcentaje conservado: {pct_conserved:.2f}%
        - Porcentaje eliminado: {pct_removed:.2f}%

        CRITERIOS DE DUPLICACI√ìN:
        - T√≠tulo normalizado (sin puntuaci√≥n, min√∫sculas)
        - DOI (si est√° disponible)
        - Autores

        ========================================
        """
        return report
    
    def save_files(self, base_filename: Optional[str] = None) -> Tuple[str, str, str]:
        """
        Guarda los archivos procesados y el reporte de limpieza.
        
        Genera y guarda tres archivos en el directorio actual:
        1. *_LIMPIO.csv: Dataset limpio sin duplicados
        2. *_COMPLETO.csv: Dataset original con columna 'removal_info'
        3. *_REPORTE.txt: Reporte de texto con estad√≠sticas
        
        Args:
            base_filename (Optional[str], optional): Nombre base para los archivos.
                Si no se proporciona, genera uno autom√°tico con timestamp.
                Ejemplo: "articles_2025" generar√°:
                - articles_2025_LIMPIO.csv
                - articles_2025_COMPLETO.csv
                - articles_2025_REPORTE.txt
        
        Returns:
            Tuple[str, str, str]: Tupla con las rutas de los tres archivos generados:
                (clean_file, full_file, report_file)
        
        Raises:
            ValueError: Si clean_data() no se ha ejecutado previamente
                (self.df_clean is None)
        
        Files Generated:
            1. **LIMPIO.csv**: 
               - Dataset final para an√°lisis
               - Sin duplicados ni registros inv√°lidos
               - Mismas columnas que el original
               - √çndices reseteados 0 a N-1
            
            2. **COMPLETO.csv**:
               - Dataset original completo
               - Nueva columna 'removal_info' indica estado de cada registro
               - √ötil para auditor√≠a y trazabilidad
               - Permite recuperar registros eliminados si es necesario
            
            3. **REPORTE.txt**:
               - Reporte de texto plano
               - Estad√≠sticas detalladas
               - Criterios de limpieza
               - Fecha y metadata del proceso
        
        Example:
            >>> cleaner = DataCleaner("raw_articles.csv")
            >>> cleaner.load_data()
            >>> cleaner.clean_data()
            >>> 
            >>> # Opci√≥n 1: Generar nombres autom√°ticos con timestamp
            >>> clean, full, report = cleaner.save_files()
            üíæ Archivo limpio guardado: ebsco_data_20250115_143045_LIMPIO.csv
            üíæ Archivo completo guardado: ebsco_data_20250115_143045_COMPLETO.csv
            üìã Reporte guardado: ebsco_data_20250115_143045_REPORTE.txt
            
            >>> # Opci√≥n 2: Especificar nombre base personalizado
            >>> clean, full, report = cleaner.save_files("ml_articles_cleaned")
            üíæ Archivo limpio guardado: ml_articles_cleaned_LIMPIO.csv
            üíæ Archivo completo guardado: ml_articles_cleaned_COMPLETO.csv
            üìã Reporte guardado: ml_articles_cleaned_REPORTE.txt
            
            >>> # Usar rutas retornadas para procesamiento posterior
            >>> import pandas as pd
            >>> df_final = pd.read_csv(clean)
            >>> print(f"Cargado dataset limpio: {len(df_final)} registros")
        
        File Formats:
            - Encoding: UTF-8 (compatible con caracteres internacionales)
            - Separator: Comma (,)
            - Index: No incluido en CSV (index=False)
            - Header: Incluido (primera fila son nombres de columnas)
        
        Best Practices:
            - Especificar base_filename descriptivo para facilitar identificaci√≥n
            - Mantener archivos _COMPLETO.csv para auditor√≠a
            - Versionar archivos si se realizan m√∫ltiples limpiezas
            - Respaldar archivos originales antes de limpieza
        
        Note:
            Los archivos se guardan en el directorio actual. Para especificar
            otra ubicaci√≥n, incluir la ruta en base_filename:
            cleaner.save_files("/home/user/data/articles")
        """
        # Verificar que clean_data() se ejecut√≥
        if self.df_clean is None:
            raise ValueError("Datos no limpiados. Ejecuta clean_data() primero.")
        
        # ===== GENERAR NOMBRE BASE SI NO SE PROPORCIONA =====
        if base_filename is None:
            # Crear timestamp en formato YYYYMMDD_HHMMSS
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            base_filename = f"ebsco_data_{timestamp}"
        
        # ===== CONSTRUIR RUTAS DE ARCHIVOS =====
        clean_file = f"{base_filename}_LIMPIO.csv"
        full_file = f"{base_filename}_COMPLETO.csv"
        report_file = f"{base_filename}_REPORTE.txt"
        
        # ===== GUARDAR ARCHIVO LIMPIO =====
        self.df_clean.to_csv(clean_file, index=False, encoding='utf-8')
        print(f"Archivo limpio guardado: {clean_file}")
        
        # ===== GUARDAR ARCHIVO COMPLETO CON INFO DE ELIMINACI√ìN =====
        df_with_info = self.create_removal_info_column()
        df_with_info.to_csv(full_file, index=False, encoding='utf-8')
        print(f"Archivo completo guardado: {full_file}")
        
        # ===== GUARDAR REPORTE DE TEXTO =====
        report = self.generate_cleaning_report()
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"Reporte guardado: {report_file}")
        
        # Retornar tupla con las tres rutas
        return clean_file, full_file, report_file
    
    def get_duplicate_analysis(self) -> pd.DataFrame:
        """
        Genera un an√°lisis detallado de los duplicados encontrados.
        
        Crea un DataFrame con informaci√≥n completa sobre cada registro que
        fue eliminado por ser duplicado, incluyendo su √≠ndice, el √≠ndice del
        registro que se mantuvo, t√≠tulos, autores y DOI de ambos.
        
        Este an√°lisis es √∫til para:
        - Verificar que la detecci√≥n de duplicados funciona correctamente
        - Auditar decisiones de eliminaci√≥n
        - Identificar patrones de duplicaci√≥n
        - Recuperar registros espec√≠ficos si es necesario
        
        Returns:
            pd.DataFrame: DataFrame con las siguientes columnas:
                - indice_eliminado: √çndice del registro que fue eliminado
                - indice_conservado: √çndice del registro que se mantuvo
                - titulo_eliminado: T√≠tulo del registro eliminado (truncado a 100 chars)
                - titulo_conservado: T√≠tulo del registro conservado (truncado)
                - autores_eliminado: Autores del registro eliminado (truncado a 50 chars)
                - doi_eliminado: DOI del registro eliminado
                - razon: Raz√≥n de eliminaci√≥n (siempre 'DUPLICADO')
                
                Retorna DataFrame vac√≠o si no hay duplicados.
        
        Example:
            >>> cleaner = DataCleaner("articles.csv")
            >>> cleaner.load_data()
            >>> cleaner.clean_data()
            >>> 
            >>> # Obtener an√°lisis de duplicados
            >>> dup_analysis = cleaner.get_duplicate_analysis()
            >>> print(f"Total de duplicados analizados: {len(dup_analysis)}")
            Total de duplicados analizados: 234
            
            >>> # Ver primeros duplicados
            >>> print(dup_analysis.head())
            indice_eliminado  indice_conservado  titulo_eliminado  ...
            42                15                 Machine Learning...
            156               15                 Machine Learning...
            789               45                 Deep Neural Netw...
            
            >>> # Verificar un caso espec√≠fico
            >>> case = dup_analysis[dup_analysis['indice_eliminado'] == 42].iloc[0]
            >>> print(f"Registro {case['indice_eliminado']} eliminado")
            >>> print(f"Era duplicado del registro {case['indice_conservado']}")
            >>> print(f"T√≠tulo eliminado: {case['titulo_eliminado']}")
            >>> print(f"T√≠tulo conservado: {case['titulo_conservado']}")
            
            >>> # Exportar an√°lisis para revisi√≥n externa
            >>> dup_analysis.to_csv("duplicados_analisis.csv", index=False)
        
        Analysis Use Cases:
            1. **Verificaci√≥n**: Confirmar que registros similares se agruparon
            2. **Recuperaci√≥n**: Encontrar registros eliminados por error
            3. **Estad√≠sticas**: Analizar patrones de duplicaci√≥n por journal, autor, etc.
            4. **Documentaci√≥n**: Evidencia detallada para informes
        
        Note:
            Los t√≠tulos y autores se truncan para mantener el DataFrame
            manejable. Para ver contenido completo, consultar df_original
            directamente usando los √≠ndices.
        """
        # Si no hay duplicados, retornar DataFrame vac√≠o
        if not self.duplicate_info:
            return pd.DataFrame()
        
        # Verificar que df_original existe
        if self.df_original is None:
            raise ValueError("df_original es None. No se puede generar an√°lisis de duplicados.")

        # Lista para almacenar datos de an√°lisis
        analysis_data = []
        
        # Iterar sobre cada duplicado eliminado
        for idx, info in self.duplicate_info.items():
            # Verificar que el √≠ndice existe en df_original
            if idx < len(self.df_original):  # type: ignore[arg-type]
                # Obtener fila del registro eliminado
                row = self.df_original.iloc[idx]  # type: ignore[index]
                
                # Extraer y truncar informaci√≥n relevante
                titulo_eliminado = str(row.get('title', ''))[:100]
                if len(str(row.get('title', ''))) > 100:
                    titulo_eliminado += "..."
                
                autores_eliminado = str(row.get('authors', ''))[:50]
                if len(str(row.get('authors', ''))) > 50:
                    autores_eliminado += "..."
                
                # Construir registro para an√°lisis
                analysis_data.append({
                    'indice_eliminado': idx,
                    'indice_conservado': info['kept_index'],
                    'titulo_eliminado': titulo_eliminado,
                    'titulo_conservado': info['duplicate_of_title'],
                    'autores_eliminado': autores_eliminado,
                    'doi_eliminado': row.get('doi', ''),
                    'razon': info['reason']
                })
        
        # Convertir lista a DataFrame
        return pd.DataFrame(analysis_data)


# ============================================================================
# FUNCI√ìN DE CONVENIENCIA
# ============================================================================

def clean_ebsco_data(input_file: str, output_base_name: Optional[str] = None) -> Tuple[str, str, str]:
    """
    Funci√≥n de conveniencia para limpiar datos de EBSCO con un solo comando.
    
    Esta funci√≥n wrapper simplifica el proceso de limpieza ejecutando
    autom√°ticamente todos los pasos necesarios: carga, limpieza, y exportaci√≥n.
    Es la forma m√°s r√°pida de limpiar un dataset si no se necesita acceso
    intermedio a los DataFrames o estad√≠sticas.
    
    Args:
        input_file (str): Ruta al archivo CSV de entrada con datos de EBSCO.
            Debe existir y ser un CSV v√°lido con encoding UTF-8.
        output_base_name (Optional[str], optional): Nombre base para archivos
            de salida. Si es None, genera nombre autom√°tico con timestamp.
            Por defecto None.
    
    Returns:
        Tuple[str, str, str]: Tupla con rutas de los tres archivos generados:
            (clean_file_path, full_file_path, report_file_path)
    
    Raises:
        FileNotFoundError: Si input_file no existe
        Exception: Si hay error al cargar los datos
    
    Process:
        1. Verificar que el archivo existe
        2. Crear instancia de DataCleaner
        3. Cargar datos con load_data()
        4. Limpiar datos con clean_data()
        5. Guardar archivos con save_files()
        6. Retornar rutas de archivos generados
    
    Example:
        >>> # Uso b√°sico - nombres autom√°ticos
        >>> clean, full, report = clean_ebsco_data("articles.csv")
        üöÄ Iniciando proceso de limpieza de datos EBSCO...
        ‚úÖ Datos cargados exitosamente: 5,432 registros
        üßπ Iniciando limpieza de datos...
        ...
        üéâ Proceso de limpieza completado exitosamente!
        üìÅ Archivos generados:
           üìÑ Datos limpios: ebsco_data_20250115_143045_LIMPIO.csv
           üìÑ Datos completos: ebsco_data_20250115_143045_COMPLETO.csv
           üìÑ Reporte: ebsco_data_20250115_143045_REPORTE.txt
        
        >>> # Uso con nombre personalizado
        >>> clean, full, report = clean_ebsco_data(
        ...     "raw_articles.csv",
        ...     output_base_name="ml_articles_2025"
        ... )
        
        >>> # Continuar con an√°lisis posterior
        >>> import pandas as pd
        >>> df = pd.read_csv(clean)
        >>> print(f"Dataset limpio: {len(df)} art√≠culos")
    
    Advanced Usage:
        Si necesitas m√°s control sobre el proceso o acceso a estad√≠sticas
        intermedias, usa la clase DataCleaner directamente:
        
        >>> cleaner = DataCleaner("articles.csv")
        >>> cleaner.load_data()
        >>> 
        >>> # Inspeccionar antes de limpiar
        >>> print(f"Registros originales: {len(cleaner.df_original)}")
        >>> 
        >>> # Limpiar
        >>> df_clean = cleaner.clean_data()
        >>> 
        >>> # Analizar duplicados
        >>> dup_analysis = cleaner.get_duplicate_analysis()
        >>> 
        >>> # Guardar
        >>> cleaner.save_files("custom_name")
    
    Best Practices:
        - Mantener backup del archivo original antes de limpiar
        - Revisar el reporte generado para verificar resultados
        - Examinar archivo _COMPLETO.csv para auditar eliminaciones
        - Usar nombres descriptivos para output_base_name
    
    Note:
        Esta funci√≥n es ideal para scripts automatizados o uso interactivo
        r√°pido. Para workflows complejos, considera usar DataCleaner directamente.
    """
    print("üöÄ Iniciando proceso de limpieza de datos EBSCO...")
    
    # ===== PASO 1: VERIFICAR EXISTENCIA DEL ARCHIVO =====
    if not os.path.exists(input_file):
        raise FileNotFoundError(f"El archivo {input_file} no existe")
    
    # ===== PASO 2: CREAR INSTANCIA DE DATACLEANER =====
    cleaner = DataCleaner(input_file)
    
    # ===== PASO 3: CARGAR DATOS =====
    if not cleaner.load_data():
        raise Exception("Error cargando los datos")
    
    # ===== PASO 4: LIMPIAR DATOS =====
    cleaner.clean_data()
    
    # ===== PASO 5: GUARDAR ARCHIVOS =====
    clean_file, full_file, report_file = cleaner.save_files(output_base_name)
    
    # ===== PASO 6: MOSTRAR RESUMEN =====
    print("üéâ Proceso de limpieza completado exitosamente!")
    print(f"üìÅ Archivos generados:")
    print(f"   üìÑ Datos limpios: {clean_file}")
    print(f"   üìÑ Datos completos: {full_file}")
    print(f"   üìÑ Reporte: {report_file}")
    
    return clean_file, full_file, report_file


# ============================================================================
# EJEMPLO DE USO
# ============================================================================

if __name__ == "__main__":
    """
    Ejemplo de uso del limpiador de datos de EBSCO.
    
    Este bloque demuestra dos formas de usar el m√≥dulo:
    1. Funci√≥n de conveniencia (r√°pida y simple)
    2. Clase DataCleaner (control completo)
    """
    
    # ========== OPCI√ìN 1: FUNCI√ìN DE CONVENIENCIA ==========
    # Forma m√°s simple - un solo comando
    print("=" * 60)
    print("OPCI√ìN 1: Usando funci√≥n de conveniencia")
    print("=" * 60)
    
    clean, full, report = clean_ebsco_data(
        "ebsco_raw_data.csv",
        output_base_name="articles_cleaned_2025"
    )
    
    # ========== OPCI√ìN 2: CLASE DATACLEANER (CONTROL TOTAL) ==========
    print("\n" + "=" * 60)
    print("OPCI√ìN 2: Usando clase DataCleaner directamente")
    print("=" * 60)
    
    # Crear instancia
    cleaner = DataCleaner("ebsco_raw_data.csv")
    
    # Cargar datos
    if cleaner.load_data():
        # Inspeccionar antes de limpiar
        df = cleaner.df_original
        # df no ser√° None porque load_data() devolvi√≥ True, pero ayudamos al type-checker
        assert df is not None
        print(f"\nDataset original: {len(df)} registros")
        print(f"Columnas: {df.columns.tolist()}")
        
        # Limpiar datos
        df_clean = cleaner.clean_data()
        
        # Obtener an√°lisis de duplicados
        dup_analysis = cleaner.get_duplicate_analysis()
        if len(dup_analysis) > 0:
            print(f"\nPrimeros 5 duplicados encontrados:")
            print(dup_analysis.head())
        
        # Guardar archivos
        clean_file, full_file, report_file = cleaner.save_files("detailed_clean")
        
        # Mostrar reporte en consola
        print("\n" + cleaner.generate_cleaning_report())